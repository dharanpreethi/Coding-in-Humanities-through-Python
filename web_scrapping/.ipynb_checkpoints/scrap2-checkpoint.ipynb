{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newspaper scraping\n",
    "\n",
    "## Imposing  structure on data\n",
    "\n",
    "* Web scraping focuses on the transformation of unstructured data on the web\n",
    "* Typically in html format, into structured data that can be stored\n",
    "* And analyzed in a central local database or spreadsheat\n",
    "\n",
    "For this lesson, we will now access BBC News website i.e https://www.bbc.co.uk/search?q=covid+19&page=1 and search about covid-19, covid-19 second wave, and vaccination related keywords in a search box present on the top of the page shown in the picture.\n",
    "\n",
    "![](../images/web3.png)\n",
    "\n",
    "\n",
    "\n",
    "Let we write the script to scrape the news by giving keywords in our python\n",
    "\n",
    "## Import the packages\n",
    "\n",
    "here the the BBC news website had written using html scripts to extract the html scripts we have to install the BeautifulSoup and requests packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request,sys,time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pages to get\n",
    " After searching of keywords the results shown in the number of pages\n",
    "\n",
    "![](../images/web4.png)\n",
    "\n",
    "so we have to intialize the variable to request the particular page and the result of the searching will be stored in the dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagesToGet= 1\n",
    "resultframe=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the keyword to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2dae5981230d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Enter the query: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m         )\n\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "key=input(\"Enter the query: \")\n",
    "key=key.replace(' ','+')\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting html script\n",
    "write the code to extract html script using class names mentioned in the bbc news website.\n",
    "\n",
    "![](../images/web5.png)\n",
    "\n",
    "and the result stored in the csv file named <b>\"search_res.csv\" </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing page : 1\n",
      "https://www.bbc.co.uk/search?q=covid+19+article&page=1\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for page in range(1,pagesToGet+1):\n",
    "    print('processing page :', page)\n",
    "    url = 'https://www.bbc.co.uk/search?q='+key+'+article&page='+str(page)\n",
    "    print(url)\n",
    "    page=requests.get(url)            \n",
    "    soup=BeautifulSoup(page.text,'html.parser')\n",
    "    frame=[]\n",
    "    links=soup.find('ul',attrs={'class':'css-1lb37cz-Stack e1y4nx260'}).find_all('li')\n",
    "    #links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
    "    print(len(links))\n",
    "    filename=\"search_res.csv\"\n",
    "    f=open(filename,\"w\", encoding = 'utf-8')\n",
    "    headers=\"Statement,Link,Date\\n\"\n",
    "    f.write(headers)\n",
    "    \n",
    "    for j in links:\n",
    "      \n",
    "        Statement = j.find(\"div\",attrs={'class':'css-l100ew-PromoContentSummary e1f5wbog1'}).find('p',attrs={'class':'css-1uw1j0b-PromoHeadline e1f5wbog2'}).find('a',attrs={'class':'css-vh7bxp-PromoLink e1f5wbog6'}).text.strip()\n",
    "       \n",
    "        Link = j.find(\"p\",attrs={'class':'css-1uw1j0b-PromoHeadline e1f5wbog2'}).find('a')['href'].strip()\n",
    "        Date = j.find('span',attrs={'class':'css-1hizfh0-MetadataSnippet ecn1o5v0'}).text[8:].strip()\n",
    "       \n",
    "        frame.append((Statement,Link,Date))\n",
    "        f.write(Statement.replace(\",\",\"^\")+\",\"+Link+\",\"+Date.replace(\",\",\"^\")+\"\\n\")\n",
    "    resultframe.extend(frame)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result csv file\n",
    "\n",
    "here in a page there are 10 links are displayed. The title of the news related to keyword and published url and published date are stored in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Statement  \\\n",
      "0  BBC Inside Science: Coronavirus conspiracy, Li...   \n",
      "1  Coronavirus: Third West Yorkshire bed factory ...   \n",
      "2  Covid: Biden vows 100m vaccinations for US in ...   \n",
      "3  US election 2020: Biden takes Georgia to solid...   \n",
      "4           Coronavirus in Sudan exposes new leaders   \n",
      "5  BCG: Can a vaccine from 1921 save lives from C...   \n",
      "6  Shoprite: Africa's biggest supermarket conside...   \n",
      "7  Coronavirus in Africa: 'Signs of hope' as case...   \n",
      "8  Coronavirus: Calls to protect 'vital' warehous...   \n",
      "9  Letter from Africa: Spare a thought for strand...   \n",
      "\n",
      "                                                Link         Date  \n",
      "0          https://www.bbc.co.uk/programmes/m000k325  18 Jun 2020  \n",
      "1  https://www.bbc.co.uk/news/uk-england-leeds-53...  13 Jul 2020  \n",
      "2  https://www.bbc.co.uk/news/world-us-canada-552...   9 Dec 2020  \n",
      "3  https://www.bbc.co.uk/news/election-us-2020-54...  14 Nov 2020  \n",
      "4   https://www.bbc.co.uk/news/world-africa-52735520  25 May 2020  \n",
      "5         https://www.bbc.co.uk/news/health-54465733  11 Oct 2020  \n",
      "6   https://www.bbc.co.uk/news/world-africa-53637506   3 Aug 2020  \n",
      "7   https://www.bbc.co.uk/news/world-africa-53847699  20 Aug 2020  \n",
      "8  https://www.bbc.co.uk/news/uk-england-south-yo...   2 Apr 2020  \n",
      "9   https://www.bbc.co.uk/news/world-africa-52645702  17 May 2020  \n"
     ]
    }
   ],
   "source": [
    "data=pd.DataFrame(resultframe, columns=['Statement','Link','Date'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Article package\n",
    "\n",
    "using the article package, we can display the various properties of news article like title of the news, summary of the news, meta description etc., we can take the link of the above result i.e data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            https://www.bbc.co.uk/programmes/m000k325\n",
       "1    https://www.bbc.co.uk/news/uk-england-leeds-53...\n",
       "2    https://www.bbc.co.uk/news/world-us-canada-552...\n",
       "3    https://www.bbc.co.uk/news/election-us-2020-54...\n",
       "4     https://www.bbc.co.uk/news/world-africa-52735520\n",
       "5           https://www.bbc.co.uk/news/health-54465733\n",
       "6     https://www.bbc.co.uk/news/world-africa-53637506\n",
       "7     https://www.bbc.co.uk/news/world-africa-53847699\n",
       "8    https://www.bbc.co.uk/news/uk-england-south-yo...\n",
       "9     https://www.bbc.co.uk/news/world-africa-52645702\n",
       "Name: Link, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installing the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (2.25.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (2.8.1)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (3.1.0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (3.5)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (5.3.1)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (4.6.2)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (6.0.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from newspaper3k) (8.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.1)\n",
      "Requirement already satisfied: six in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.50.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.2)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\shanmugapriya\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = data['Link'][1] #for example take a first result link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply parsing to know the properties of news article easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_article = Article(url, language=\"en\") # en for English \n",
    "res_article.download()  #download an article\n",
    "res_article.parse() #To parse the article \n",
    "res_article.nlp() #To perform natural language processing ie..nlp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying the title of the news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coronavirus: Third West Yorkshire bed factory outbreak'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_article.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying the text of the news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"What interests me a lot more is how people get to and from work, because actually you find a lot of people are car-sharing and in those scenarios you\\'re in quite close contact with others for quite a long period of time, dependent on the commute.\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_article.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using BeautifulSoup to extract entire article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.bbc.co.uk/news/uk-england-leeds-53394717'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=data['Link'][2]\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/web6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bed factory has seen eight workers test positive for coronavirus - the third in a series of outbreaks at similar sites in West Yorkshire.\n",
      "Highgrove Beds in Liversedge ceased production as a safety precaution with all staff being offered tests.\n",
      "The outbreak follows cases at Deep Sleep Beds in Ossett and Dura Beds in Batley over the past month.\n",
      "There have also been cases of coronavirus reported at two meat factories in West Yorkshire.\n",
      "Rachel Spencer-Henshall, director of public health at Kirklees Council, warned factory workers of the risk of car sharing.\n",
      "She said: \"With the bed factories, it's less about the industry itself and more about working in a factory setting.\n",
      "\"What interests me a lot more is how people get to and from work, because actually you find a lot of people are car-sharing and in those scenarios you're in quite close contact with others for quite a long period of time, dependent on the commute.\"\n",
      "Latest news and stories from YorkshireDeep Sleep Beds in Ossett sees four casesDura Beds in Batley: Eight staff contract Covid-19\n",
      "Kirklees Council have been working with Highgrove Beds for about five days with an increase in cases seen over the weekend.\n",
      "The factory has been was inspected by Kirklees Council and \"presented high standards of infection control procedures and practices\".\n",
      "In a joint statement, the company, the council and Public Health England said the risk to local residents from the outbreak was very low.\n",
      "A SIMPLE GUIDE:  How do I protect myself?IMPACT: What the virus does to the bodyRECOVERY: How long does it take?LOCKDOWN: How can we lift restrictions?ENDGAME: How do we get out of this mess?\n",
      "Earlier this month, four workers at Deep Sleep Beds tested positive for Covid-19.\n",
      "On 1 July, it was revealed that eight staff at Dura Beds had contracted the virus.\n",
      "This comes as Forza Foods in Normanton reported 17 positive cases. \n",
      "Last month it was confirmed that 165 staff working at the Kober factory in Cleckheaton had tested positive for the virus.\n",
      "Why are there outbreaks in meat processing plants?Forza Foods open despite coronavirus cases'Secrecy' claims over Cleckheaton outbreak\n",
      "Ms Spencer-Henshall said that certain industries have seen increases in cases including meat processing. \n",
      "She added: \"Those factory conditions can be really attractive to the virus, particularly being a cold environment and quite loud so people are having to communicate and shout. \n",
      "\"We all know that singing is frowned upon at the moment and it's a similar thing in terms of projecting the virus.\"\n",
      "Follow BBC Yorkshire on Facebook, Twitter and Instagram. Send your story ideas to yorkslincs.news@bbc.co.uk or send video here.\n"
     ]
    }
   ],
   "source": [
    "page=requests.get(url)\n",
    "     \n",
    "soup=BeautifulSoup(page.text,'html.parser')\n",
    "frame=[]\n",
    "links=soup.find('article',attrs={'class':'css-5h7eao-ArticleWrapper e1nh2i2l0'}).find_all('div',attrs={'class':'css-uf6wea-RichTextComponentWrapper e1xue1i83'})\n",
    "#links=soup.find_all('li',attrs={'class':'o-listicle__item'})\n",
    "#print(\"l:\"+links)\n",
    "for i in links:\n",
    "    news=i.find('div',attrs={'class':'css-83cqas-RichTextContainer e5tfeyi2'}).text\n",
    "    print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
